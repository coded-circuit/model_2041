# Model 2041 - Vision-Language Grounding System

A comprehensive vision-language AI system for object detection, grounding, and visual question answering (VQA) using YOLO, Qwen3-VL, and custom LoRA adapters.
---

## ğŸ¯ Overview

This project implements a multi-modal AI system that combines:
- **Object Detection**: YOLO11n-OBB with custom LoRA weights for oriented bounding box detection
- **Vision-Language Understanding**: Qwen3-VL-32B for semantic understanding and coordinate refinement
- **Image Classification**: Automatic classification of SAR, Optical, and FCC images
- **Query Classification**: Intelligent routing of queries to VQA or Grounding tasks
- **RESTful API**: Flask-based API server for production deployment

---

## ğŸ“ Repository Structure

```
model_2041-main/
â”‚
â”œâ”€â”€ Final_Grounding/              # Main grounding pipeline module
â”‚   â”œâ”€â”€ main.py                   # Entry point for grounding pipeline
â”‚   â”œâ”€â”€ requirements.txt          # Dependencies for grounding module
â”‚   â”œâ”€â”€ yolo11n-obb.pt           # YOLO base model weights (Git LFS)
â”‚   â”œâ”€â”€ lora_weights.pt          # LoRA weights for YOLO (Git LFS)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ detector.py          # YOLO+LoRA detector implementation
â”‚       â”œâ”€â”€ lora.py              # LoRA layer implementation
â”‚       â”œâ”€â”€ pipeline.py          # Main pipeline orchestrator
â”‚       â””â”€â”€ refiner.py           # Qwen3-VL coordinate refiner
â”‚
â”œâ”€â”€ testing/                      # API server and inference pipeline
â”‚   â”œâ”€â”€ api_server.py            # Flask API server (main entry point)
â”‚   â”œâ”€â”€ inference.py             # Main inference pipeline orchestrator
â”‚   â”œâ”€â”€ model_loading.py         # Model loading utilities
â”‚   â”œâ”€â”€ classifier.py            # Image type classifier (SAR/Optical/FCC)
â”‚   â”œâ”€â”€ query_classifier.py      # Query type classifier (VQA/Grounding)
â”‚   â”œâ”€â”€ vqa_inference.py         # Visual Question Answering module
â”‚   â”œâ”€â”€ geo_ground.py            # Geographic grounding module
â”‚   â”œâ”€â”€ fcc.py                   # FCC-specific processing
â”‚   â”œâ”€â”€ adapter_changing.py      # LoRA adapter switching utilities
â”‚   â”œâ”€â”€ requirements_api.txt     # API server dependencies
â”‚   â”œâ”€â”€ README_DEPLOY.md         # Deployment documentation
â”‚   â”œâ”€â”€ DEPLOYMENT_CHECKLIST.md  # Deployment checklist
â”‚   â”œâ”€â”€ SAR_LORA_ADAPTER/        # SAR-specific LoRA adapter (Git LFS)
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â””â”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ Qwen_for_Optical_2/      # Optical-specific LoRA adapter (Git LFS)
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ adapter_model.safetensors
â”‚
â”œâ”€â”€ temp/                         # Development and training files
â”‚   â”œâ”€â”€ classifiers/
â”‚   â”‚   â”œâ”€â”€ final_model_all_classes.keras  # Image classifier model
â”‚   â”‚   â””â”€â”€ query_classifer.py            # Query classifier implementation
â”‚   â”œâ”€â”€ fcc model/
â”‚   â”‚   â””â”€â”€ fcc best model.pt              # FCC model weights
â”‚   â”œâ”€â”€ fcc.ipynb                 # FCC training notebook
â”‚   â”œâ”€â”€ image_classifier.ipynb    # Image classifier training notebook
â”‚   â”œâ”€â”€ Multimodal.ipynb         # Multimodal training notebook
â”‚   â””â”€â”€ qwen_lora.py             # LoRA adapter utilities
â”‚
â”œâ”€â”€ optical.py                    # Optical image inference utility
â”œâ”€â”€ Dockerfile                    # Docker container configuration
â”œâ”€â”€ .gitattributes               # Git LFS configuration
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”„ File Flow & Architecture

### Main Pipeline Flow

```
User Input (Image + Query)
    â”‚
    â”œâ”€â†’ Image Classification (SAR/Optical/FCC)
    â”‚   â””â”€â†’ classifier.py â†’ Select appropriate LoRA adapter
    â”‚
    â”œâ”€â†’ Query Classification (VQA/Grounding)
    â”‚   â””â”€â†’ query_classifier.py â†’ Route to appropriate handler
    â”‚
    â””â”€â†’ Task Execution
        â”‚
        â”œâ”€â†’ VQA Path
        â”‚   â””â”€â†’ vqa_inference.py
        â”‚       â””â”€â†’ Qwen3-VL with image-specific LoRA
        â”‚           â””â”€â†’ Text response
        â”‚
        â””â”€â†’ Grounding Path
            â””â”€â†’ geo_ground.py / Final_Grounding/
                â”œâ”€â†’ YOLO Detection (detector.py)
                â”‚   â””â”€â†’ Initial bounding boxes with LoRA
                â”‚
                â””â”€â†’ Qwen3-VL Refinement (refiner.py)
                    â””â”€â†’ Refined coordinates matching query
                        â””â”€â†’ Final coordinates [cx, cy, w, h, angle]
```

### Detailed Component Flow

#### 1. **API Server Flow** (`testing/api_server.py`)
```
POST /infer
    â”‚
    â”œâ”€â†’ Download image (if URL provided)
    â”œâ”€â†’ Initialize InferencePipeline (singleton)
    â”‚   â””â”€â†’ Load all models once (cached in memory)
    â”‚
    â””â”€â†’ pipeline.run(image_path, query)
        â”‚
        â”œâ”€â†’ Classify Image Type
        â”‚   â””â”€â†’ Load appropriate LoRA adapter
        â”‚
        â”œâ”€â†’ Classify Query Type
        â”‚   â””â”€â†’ Determine task (VQA/Grounding)
        â”‚
        â””â”€â†’ Execute Task
            â”œâ”€â†’ VQA: Generate text response
            â””â”€â†’ Grounding: Return coordinates
```

#### 2. **Grounding Pipeline Flow** (`Final_Grounding/`)
```
initialize_pipeline()
    â”‚
    â”œâ”€â†’ Load YOLO Model (yolo11n-obb.pt)
    â”œâ”€â†’ Inject LoRA Layers (lora.py)
    â”œâ”€â†’ Load LoRA Weights (lora_weights.pt)
    â””â”€â†’ Load Qwen3-VL Refiner (refiner.py)

get_coordinates(image_path, query)
    â”‚
    â”œâ”€â†’ YOLO Detection (detector.py)
    â”‚   â””â”€â†’ Returns: [[cx, cy, w, h, angle], ...]
    â”‚
    â””â”€â†’ Qwen3-VL Refinement (refiner.py)
        â”œâ”€â†’ Format coordinates as prompt
        â”œâ”€â†’ Process image + query + coordinates
        â””â”€â†’ Returns: Filtered coordinates matching query
```

#### 3. **Model Loading Flow** (`testing/model_loading.py`)
```
initialize()
    â”‚
    â”œâ”€â†’ Load Qwen3-VL Base Model (quantized 4-bit)
    â”œâ”€â†’ Load Image Classifier (Keras model)
    â”œâ”€â†’ Load Query Classifier
    â”œâ”€â†’ Load SAR LoRA Adapter
    â”œâ”€â†’ Load Optical LoRA Adapter
    â””â”€â†’ Load FCC Model (if needed)
```

## ğŸ”§ Prerequisites

- **Python**: 3.8 or higher
- **CUDA**: 11.8+ (for GPU acceleration)
- **GPU**: NVIDIA GPU with at least 16GB VRAM (recommended)
- **Git LFS**: Required for downloading large model files
- **Operating System**: Linux (recommended), Windows, or macOS

### System Requirements
- **RAM**: 32GB+ recommended
- **Storage**: 50GB+ free space for models
- **CUDA Toolkit**: 12.1+ (if using GPU)

---

## ğŸš€ Installation & Setup

### Step 1: Clone the Repository

```bash
# Clone the repository
git clone <repository-url>
cd model_2041-main

# Initialize Git LFS (required for model files)
git lfs install
git lfs pull
```

### Step 2: Create Virtual Environment

**On Linux/macOS:**
```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate
```

**On Windows:**
```powershell
# Create virtual environment
python -m venv venv

# Activate virtual environment
venv\Scripts\activate
```

### Step 3: Install Dependencies

**For API Server (Recommended):**
```bash
# Install API server dependencies
pip install --upgrade pip
pip install -r testing/requirements_api.txt
```

**For Grounding Module Only:**
```bash
# Install grounding module dependencies
pip install --upgrade pip
pip install -r Final_Grounding/requirements.txt
```

**Note**: The API server requirements include all dependencies needed for the full system.

### Step 4: Verify Installation

```bash
# Check Python version
python --version  # Should be 3.8+

# Check CUDA (if using GPU)
python -c "import torch; print(torch.cuda.is_available())"
```

### Step 5: Download Model Files (if not using Git LFS)

If model files are not automatically downloaded via Git LFS, ensure the following files exist:

- `Final_Grounding/yolo11n-obb.pt`
- `Final_Grounding/lora_weights.pt`
- `testing/SAR_LORA_ADAPTER/adapter_model.safetensors`
- `testing/Qwen_for_Optical_2/adapter_model.safetensors`
- `temp/classifiers/final_model_all_classes.keras`

---

## ğŸ’» Usage

### Option 1: Run API Server (Recommended)

The API server provides a RESTful interface for all functionality.

```bash
# Navigate to testing directory
cd testing

# Run the API server
python api_server.py
```

The server will start on `http://0.0.0.0:7860`

**Example API Request:**
```bash
curl -X POST http://localhost:7860/infer \
  -H "Content-Type: application/json" \
  -d '{
    "imageUrl": "path/to/image.jpg",
    "query": "Locate all the airplanes"
  }'
```

### Option 2: Use Grounding Module Directly

```python
from Final_Grounding.main import initialize_pipeline, get_coordinates

# Initialize pipeline (loads models)
initialize_pipeline(
    yolo_path="Final_Grounding/yolo11n-obb.pt",
    lora_path="Final_Grounding/lora_weights.pt"
)

# Get coordinates for an image and query
coords = get_coordinates(
    image_path="path/to/image.jpg",
    query="Locate all the airplanes",
    conf_threshold=0.2
)

print(f"Found {len(coords)} objects:")
for coord in coords:
    print(coord)
```

### Option 3: Use Optical Inference Utility

```python
from optical import inference_streaming_optimized

# Requires model and tokenizer to be loaded separately
result = inference_streaming_optimized(
    model=model,
    tokenizer=tokenizer,
    image_path="path/to/image.jpg",
    query="What is in this image?",
    max_new_tokens=512
)
print(result)
```

---

## ğŸ§© Project Components

### Core Modules

#### 1. **Final_Grounding/** - Object Grounding Pipeline
- **Purpose**: Detect and ground objects in images using YOLO + Qwen3-VL
- **Key Files**:
  - `main.py`: Simple interface for coordinate extraction
  - `src/pipeline.py`: Main orchestrator class
  - `src/detector.py`: YOLO detection with LoRA
  - `src/refiner.py`: Qwen3-VL coordinate refinement
  - `src/lora.py`: LoRA layer implementation

#### 2. **testing/** - API Server & Inference Pipeline
- **Purpose**: Production-ready API server with full pipeline
- **Key Files**:
  - `api_server.py`: Flask REST API server
  - `inference.py`: Main inference pipeline orchestrator
  - `model_loading.py`: Model initialization and caching
  - `classifier.py`: Image type classification
  - `query_classifier.py`: Query routing
  - `vqa_inference.py`: Visual Question Answering
  - `geo_ground.py`: Geographic grounding

#### 3. **temp/** - Development & Training
- **Purpose**: Training notebooks and development files
- **Contents**: Jupyter notebooks for model training and experimentation

### Model Files

All model files are stored in Git LFS. Ensure Git LFS is installed and initialized.

- **YOLO Model**: `Final_Grounding/yolo11n-obb.pt`
- **LoRA Weights**: `Final_Grounding/lora_weights.pt`
- **SAR Adapter**: `testing/SAR_LORA_ADAPTER/`
- **Optical Adapter**: `testing/Qwen_for_Optical_2/`
- **Image Classifier**: `temp/classifiers/final_model_all_classes.keras`
---
t Updated**: 2025
