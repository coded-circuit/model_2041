# Model 2041 - Vision-Language Grounding System

A comprehensive vision-language AI system for object detection, grounding, and visual question answering (VQA) using YOLO, Qwen3-VL, and custom LoRA adapters.
---

## ğŸ¯ Overview

This project implements a multi-modal AI system that combines:
- *Object Detection*: YOLO11n-OBB with custom LoRA weights for oriented bounding box detection
- *Vision-Language Understanding*: Qwen3-VL-32B for semantic understanding and coordinate refinement
- *Image Classification*: Automatic classification of SAR, Optical, and FCC images
- *Query Classification*: Intelligent routing of queries to VQA or Grounding tasks
- *RESTful API*: Flask-based API server for production deployment

---

## ğŸ“ Repository Structure


model_2041-main/
â”‚
â”œâ”€â”€ Final_Grounding/              # Main grounding pipeline module
â”‚   â”œâ”€â”€ main.py                   # Entry point for grounding pipeline
â”‚   â”œâ”€â”€ requirements.txt          # Dependencies for grounding module
â”‚   â”œâ”€â”€ yolo11n-obb.pt           # YOLO base model weights (Git LFS)
â”‚   â”œâ”€â”€ lora_weights.pt          # LoRA weights for YOLO (Git LFS)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ detector.py          # YOLO+LoRA detector implementation
â”‚       â”œâ”€â”€ lora.py              # LoRA layer implementation
â”‚       â”œâ”€â”€ pipeline.py          # Main pipeline orchestrator
â”‚       â””â”€â”€ refiner.py           # Qwen3-VL coordinate refiner
â”‚
â”œâ”€â”€ testing/                      # API server and inference pipeline
â”‚   â”œâ”€â”€ api_server.py            # Flask API server (main entry point)
â”‚   â”œâ”€â”€ inference.py             # Main inference pipeline orchestrator
â”‚   â”œâ”€â”€ model_loading.py         # Model loading utilities
â”‚   â”œâ”€â”€ classifier.py            # Image type classifier (SAR/Optical/FCC)
â”‚   â”œâ”€â”€ query_classifier.py      # Query type classifier (VQA/Grounding)
â”‚   â”œâ”€â”€ vqa_inference.py         # Visual Question Answering module
â”‚   â”œâ”€â”€ geo_ground.py            # Geographic grounding module
â”‚   â”œâ”€â”€ fcc.py                   # FCC-specific processing
â”‚   â”œâ”€â”€ adapter_changing.py      # LoRA adapter switching utilities
â”‚   â”œâ”€â”€ requirements_api.txt     # API server dependencies
â”‚   â”œâ”€â”€ README_DEPLOY.md         # Deployment documentation
â”‚   â”œâ”€â”€ DEPLOYMENT_CHECKLIST.md  # Deployment checklist
â”‚   â”œâ”€â”€ SAR_LORA_ADAPTER/        # SAR-specific LoRA adapter (Git LFS)
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â””â”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ Qwen_for_Optical_2/      # Optical-specific LoRA adapter (Git LFS)
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ adapter_model.safetensors
â”‚
â”œâ”€â”€ temp/                         # Development and training files
â”‚   â”œâ”€â”€ classifiers/
â”‚   â”‚   â”œâ”€â”€ final_model_all_classes.keras  # Image classifier model
â”‚   â”‚   â””â”€â”€ query_classifer.py            # Query classifier implementation
â”‚   â”œâ”€â”€ fcc model/
â”‚   â”‚   â””â”€â”€ fcc best model.pt              # FCC model weights
â”‚   â”œâ”€â”€ fcc.ipynb                 # FCC training notebook
â”‚   â”œâ”€â”€ image_classifier.ipynb    # Image classifier training notebook
â”‚   â”œâ”€â”€ Multimodal.ipynb         # Multimodal training notebook
â”‚   â””â”€â”€ qwen_lora.py             # LoRA adapter utilities
â”‚
â”œâ”€â”€ optical.py                    # Optical image inference utility
â”œâ”€â”€ Dockerfile                    # Docker container configuration
â”œâ”€â”€ .gitattributes               # Git LFS configuration
â””â”€â”€ README.md                    # This file


---

## ğŸ”„ File Flow & Architecture

### Main Pipeline Flow


User Input (Image + Query)
    â”‚
    â”œâ”€â†’ Image Classification (SAR/Optical/FCC)
    â”‚   â””â”€â†’ classifier.py â†’ Select appropriate LoRA adapter
    â”‚
    â”œâ”€â†’ Query Classification (VQA/Grounding)
    â”‚   â””â”€â†’ query_classifier.py â†’ Route to appropriate handler
    â”‚
    â””â”€â†’ Task Execution
        â”‚
        â”œâ”€â†’ VQA Path
        â”‚   â””â”€â†’ vqa_inference.py
        â”‚       â””â”€â†’ Qwen3-VL with image-specific LoRA
        â”‚           â””â”€â†’ Text response
        â”‚
        â””â”€â†’ Grounding Path
            â””â”€â†’ geo_ground.py / Final_Grounding/
                â”œâ”€â†’ YOLO Detection (detector.py)
                â”‚   â””â”€â†’ Initial bounding boxes with LoRA
                â”‚
                â””â”€â†’ Qwen3-VL Refinement (refiner.py)
                    â””â”€â†’ Refined coordinates matching query
                        â””â”€â†’ Final coordinates [cx, cy, w, h, angle]


### Detailed Component Flow

#### 1. *API Server Flow* (testing/api_server.py)

POST /infer
    â”‚
    â”œâ”€â†’ Download image (if URL provided)
    â”œâ”€â†’ Initialize InferencePipeline (singleton)
    â”‚   â””â”€â†’ Load all models once (cached in memory)
    â”‚
    â””â”€â†’ pipeline.run(image_path, query)
        â”‚
        â”œâ”€â†’ Classify Image Type
        â”‚   â””â”€â†’ Load appropriate LoRA adapter
        â”‚
        â”œâ”€â†’ Classify Query Type
        â”‚   â””â”€â†’ Determine task (VQA/Grounding)
        â”‚
        â””â”€â†’ Execute Task
            â”œâ”€â†’ VQA: Generate text response
            â””â”€â†’ Grounding: Return coordinates


#### 2. *Grounding Pipeline Flow* (Final_Grounding/)

initialize_pipeline()
    â”‚
    â”œâ”€â†’ Load YOLO Model (yolo11n-obb.pt)
    â”œâ”€â†’ Inject LoRA Layers (lora.py)
    â”œâ”€â†’ Load LoRA Weights (lora_weights.pt)
    â””â”€â†’ Load Qwen3-VL Refiner (refiner.py)

get_coordinates(image_path, query)
    â”‚
    â”œâ”€â†’ YOLO Detection (detector.py)
    â”‚   â””â”€â†’ Returns: [[cx, cy, w, h, angle], ...]
    â”‚
    â””â”€â†’ Qwen3-VL Refinement (refiner.py)
        â”œâ”€â†’ Format coordinates as prompt
        â”œâ”€â†’ Process image + query + coordinates
        â””â”€â†’ Returns: Filtered coordinates matching query


#### 3. *Model Loading Flow* (testing/model_loading.py)

initialize()
    â”‚
    â”œâ”€â†’ Load Qwen3-VL Base Model (quantized 4-bit)
    â”œâ”€â†’ Load Image Classifier (Keras model)
    â”œâ”€â†’ Load Query Classifier
    â”œâ”€â†’ Load SAR LoRA Adapter
    â”œâ”€â†’ Load Optical LoRA Adapter
    â””â”€â†’ Load FCC Model (if needed)


### Output Formats

#### VQA Output
json
{
    "status": "success",
    "response": "Generated text answer to the visual question"
}


#### Grounding Output
json
{
    "status": "success",
    "response": {
        "yolo_coordinates": [[cx, cy, w, h, angle], ...],
        "final_coordinates": [[cx, cy, w, h, angle], ...],
        "num_detections": 3
    }
}


*Coordinate Format:*
- cx, cy: Normalized center coordinates (0-1)
- w, h: Normalized width and height (0-1)
- angle: Rotation angle in degrees (-90 to 0)

---

## ğŸ”§ Prerequisites

- *Python*: 3.8 or higher
- *CUDA*: 11.8+ (for GPU acceleration)
- *GPU*: NVIDIA GPU with at least 16GB VRAM (recommended)
- *Git LFS*: Required for downloading large model files
- *Operating System*: Linux (recommended), Windows, or macOS

### System Requirements
- *RAM*: 32GB+ recommended
- *Storage*: 50GB+ free space for models
- *CUDA Toolkit*: 12.1+ (if using GPU)

---

## ğŸš€ Installation & Setup

### Step 1: Clone the Repository

bash
# Clone the repository
git clone <repository-url>
cd model_2041-main

# Initialize Git LFS (required for model files)
git lfs install
git lfs pull


### Step 2: Create Virtual Environment

*On Linux/macOS:*
bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate


*On Windows:*
powershell
# Create virtual environment
python -m venv venv

# Activate virtual environment
venv\Scripts\activate


### Step 3: Install Dependencies

*For API Server (Recommended):*
bash
# Install API server dependencies
pip install --upgrade pip
pip install -r testing/requirements_api.txt


*For Grounding Module Only:*
bash
# Install grounding module dependencies
pip install --upgrade pip
pip install -r Final_Grounding/requirements.txt


*Note*: The API server requirements include all dependencies needed for the full system.

### Step 4: Verify Installation

bash
# Check Python version
python --version  # Should be 3.8+

# Check CUDA (if using GPU)
python -c "import torch; print(torch.cuda.is_available())"


### Step 5: Download Model Files (if not using Git LFS)

If model files are not automatically downloaded via Git LFS, ensure the following files exist:

- Final_Grounding/yolo11n-obb.pt
- Final_Grounding/lora_weights.pt
- testing/SAR_LORA_ADAPTER/adapter_model.safetensors
- testing/Qwen_for_Optical_2/adapter_model.safetensors
- temp/classifiers/final_model_all_classes.keras

---

## ğŸ’» Usage

### Option 1: Run API Server (Recommended)

The API server provides a RESTful interface for all functionality.

bash
# Navigate to testing directory
cd testing

# Run the API server
python api_server.py


The server will start on http://0.0.0.0:7860

*Example API Request:*
bash
curl -X POST http://localhost:7860/infer \
  -H "Content-Type: application/json" \
  -d '{
    "imageUrl": "path/to/image.jpg",
    "query": "Locate all the airplanes"
  }'


### Option 2: Use Grounding Module Directly

python
from Final_Grounding.main import initialize_pipeline, get_coordinates

# Initialize pipeline (loads models)
initialize_pipeline(
    yolo_path="Final_Grounding/yolo11n-obb.pt",
    lora_path="Final_Grounding/lora_weights.pt"
)

# Get coordinates for an image and query
coords = get_coordinates(
    image_path="path/to/image.jpg",
    query="Locate all the airplanes",
    conf_threshold=0.2
)

print(f"Found {len(coords)} objects:")
for coord in coords:
    print(coord)  # [cx, cy, w, h, angle]


### Option 3: Use Optical Inference Utility

python
from optical import inference_streaming_optimized

# Requires model and tokenizer to be loaded separately
result = inference_streaming_optimized(
    model=model,
    tokenizer=tokenizer,
    image_path="path/to/image.jpg",
    query="What is in this image?",
    max_new_tokens=512
)
print(result)


---

## ğŸ§© Project Components

### Core Modules

#### 1. *Final_Grounding/* - Object Grounding Pipeline
- *Purpose*: Detect and ground objects in images using YOLO + Qwen3-VL
- *Key Files*:
  - main.py: Simple interface for coordinate extraction
  - src/pipeline.py: Main orchestrator class
  - src/detector.py: YOLO detection with LoRA
  - src/refiner.py: Qwen3-VL coordinate refinement
  - src/lora.py: LoRA layer implementation

#### 2. *testing/* - API Server & Inference Pipeline
- *Purpose*: Production-ready API server with full pipeline
- *Key Files*:
  - api_server.py: Flask REST API server
  - inference.py: Main inference pipeline orchestrator
  - model_loading.py: Model initialization and caching
  - classifier.py: Image type classification
  - query_classifier.py: Query routing
  - vqa_inference.py: Visual Question Answering
  - geo_ground.py: Geographic grounding

#### 3. *temp/* - Development & Training
- *Purpose*: Training notebooks and development files
- *Contents*: Jupyter notebooks for model training and experimentation

### Model Files

All model files are stored in Git LFS. Ensure Git LFS is installed and initialized.

- *YOLO Model*: Final_Grounding/yolo11n-obb.pt
- *LoRA Weights*: Final_Grounding/lora_weights.pt
- *SAR Adapter*: testing/SAR_LORA_ADAPTER/
- *Optical Adapter*: testing/Qwen_for_Optical_2/
- *Image Classifier*: temp/classifiers/final_model_all_classes.keras

---

## ğŸ³ Docker Deployment

### Build Docker Image

bash
# Build the Docker image
docker build -t model-2041:latest .


### Run Docker Container

bash
# Run with GPU support (NVIDIA Docker required)
docker run --gpus all -p 7860:7860 model-2041:latest

# Or without GPU (CPU only, slower)
docker run -p 7860:7860 model-2041:latest


### Docker Compose (Optional)

Create a docker-compose.yml:

yaml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "7860:7860"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0,1


Run with:
bash
docker-compose up -d


---

## ğŸŒ API Endpoints

### Health Check
http
GET /health

*Response:*
json
{"status": "ok"}


### Main Inference Endpoint
http
POST /infer
Content-Type: application/json

{
  "imageUrl": "path/to/image.jpg" or "https://example.com/image.jpg",
  "query": "Your question or grounding query"
}


*Response:*
json
{
  "status": "success",
  "response": "Result based on query type"
}


### Debug Endpoints

#### Query Classification
http
POST /classify_query
Content-Type: application/json

{
  "query": "What is in this image?"
}


#### Image Classification
http
POST /classify_image
Content-Type: application/json

{
  "imageUrl": "path/to/image.jpg"
}


---

## ğŸ“Š Model Files

### Required Model Files

| File | Location | Size (approx) | Description |
|------|----------|---------------|-------------|
| yolo11n-obb.pt | Final_Grounding/ | ~6MB | YOLO11n Oriented Bounding Box model |
| lora_weights.pt | Final_Grounding/ | ~50MB | LoRA weights for YOLO |
| adapter_model.safetensors | testing/SAR_LORA_ADAPTER/ | ~500MB | SAR-specific LoRA adapter |
| adapter_model.safetensors | testing/Qwen_for_Optical_2/ | ~500MB | Optical-specific LoRA adapter |
| final_model_all_classes.keras | temp/classifiers/ | ~50MB | Image classifier model |

### Model Downloads

Models are automatically downloaded via Git LFS. If manual download is needed:

1. *Qwen3-VL Base Model*: Automatically downloaded from HuggingFace on first use
   - Model: unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit
   - Requires HuggingFace authentication (if model is gated)

2. *YOLO Model*: Included in repository via Git LFS

3. *LoRA Adapters*: Included in repository via Git LFS

---

## ğŸ” Troubleshooting

### Common Issues

#### 1. Git LFS Files Not Downloaded
bash
# Reinstall Git LFS
git lfs install
git lfs pull


#### 2. CUDA Out of Memory
- Reduce batch size in model loading
- Use CPU mode (slower but works)
- Close other GPU applications

#### 3. Model Loading Errors
- Ensure all model files are present
- Check file permissions
- Verify CUDA compatibility

#### 4. Import Errors
bash
# Reinstall dependencies
pip install --upgrade -r testing/requirements_api.txt


#### 5. Port Already in Use
bash
# Change port in api_server.py
app.run(host='0.0.0.0', port=8080)  # Use different port


### GPU Configuration

To specify GPU devices, modify testing/api_server.py:

python
pipeline = InferencePipeline(qwen_gpu=0, geoground_gpu=1)


### Memory Optimization

For systems with limited memory:
- Use CPU mode (set device to "cpu")
- Reduce model quantization (if supported)
- Process images sequentially

---

## ğŸ“ Additional Information

### Environment Variables

Optional environment variables:

bash
# HuggingFace token (if models are gated)
export HF_TOKEN="your_token_here"

# CUDA device selection
export CUDA_VISIBLE_DEVICES=0,1

# Model cache directory
export TRANSFORMERS_CACHE="/path/to/cache"


### Performance Tips

1. *First Request*: Slow (models loading)
2. *Subsequent Requests*: Fast (models cached in memory)
3. *GPU Acceleration*: Significantly faster than CPU
4. *Batch Processing*: Not currently supported (process sequentially)

### Supported Image Formats

- JPEG (.jpg, .jpeg)
- PNG (.png)
- TIFF (.tiff, .tif)
- Other formats supported by PIL/Pillow

### Query Types

*VQA Queries:*
- "What is in this image?"
- "Describe the scene"
- "How many objects are there?"

*Grounding Queries:*
- "Locate all the airplanes"
- "Find the largest building"
- "Where are the ships?"

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (git checkout -b feature/amazing-feature)
3. Commit your changes (git commit -m 'Add amazing feature')
4. Push to the branch (git push origin feature/amazing-feature)
5. Open a Pull Request

### Development Setup

For development, install additional tools:

bash
pip install pytest black flake8 mypy


---

## ğŸ“„ License

[Specify your license here]

---

## ğŸ™ Acknowledgments

- *YOLO*: Ultralytics YOLO11
- *Qwen3-VL*: Alibaba Cloud Qwen Team
- *Unsloth*: For optimized model loading
- *HuggingFace*: For model hosting and transformers library

---

## ğŸ“§ Contact & Support

For issues, questions, or contributions, please open an issue on the repository.

---

*LastÂ Updated*:Â 2025
