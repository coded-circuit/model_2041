{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQH85rVwjhRk"
      },
      "outputs": [],
      "source": [
        "def loading():\n",
        "\n",
        "    # Classification Model loading\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import load_model\n",
        "    CLASSIFICATION_MODEL_PATH = \"/content/drive/MyDrive/final_model_all_classes.keras\"\n",
        "\n",
        "    # Load the saved model with custom layer\n",
        "    @tf.keras.utils.register_keras_serializable(package=\"custom_layers\")\n",
        "    class HistogramLayer(tf.keras.layers.Layer):\n",
        "        def __init__(self, nbins=256, value_range=(0.0, 255.0), **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.nbins = int(nbins)\n",
        "            self.value_range = (float(value_range[0]), float(value_range[1]))\n",
        "\n",
        "        def call(self, inputs):\n",
        "            x = tf.cast(inputs, tf.float32)\n",
        "            gray = tf.image.rgb_to_grayscale(x)\n",
        "\n",
        "            def per_image(img):\n",
        "                img = tf.reshape(img, [-1])\n",
        "                hist = tf.histogram_fixed_width(img, self.value_range, nbins=self.nbins)\n",
        "                hist = tf.cast(hist, tf.float32)\n",
        "                s = tf.reduce_sum(hist)\n",
        "                return tf.cond(s > 0, lambda: hist / s, lambda: hist)\n",
        "\n",
        "            return tf.map_fn(per_image, gray, fn_output_signature=tf.float32)\n",
        "\n",
        "        def get_config(self):\n",
        "            cfg = super().get_config()\n",
        "            cfg.update({\"nbins\": self.nbins, \"value_range\": self.value_range})\n",
        "            return cfg\n",
        "\n",
        "    image_classifier = load_model(CLASSIFICATION_MODEL_PATH, compile=False, custom_objects={'HistogramLayer': HistogramLayer})\n",
        "    return image_classifier\n",
        "\n",
        "\n",
        "    #architecture of fcc convertor style_transformer\n",
        "\n",
        "    class PatchEmbed(nn.Module):\n",
        "        def __init__(self, in_ch, emb, patch):\n",
        "            super().__init__()\n",
        "            self.proj = nn.Conv2d(in_ch, emb, kernel_size=patch, stride=patch)\n",
        "        def forward(self, x):\n",
        "            x = self.proj(x)\n",
        "            B, E, H, W = x.shape\n",
        "            tokens = x.flatten(2).transpose(1,2)\n",
        "            return tokens, (H, W)\n",
        "\n",
        "    class PatchUnembed(nn.Module):\n",
        "            def __init__(self, emb, out_ch, patch):\n",
        "                super().__init__()\n",
        "                self.deproj = nn.ConvTranspose2d(emb, out_ch, kernel_size=patch, stride=patch)\n",
        "            def forward(self, tokens, hw):\n",
        "                B, N, E = tokens.shape\n",
        "                H, W = hw\n",
        "                x = tokens.transpose(1,2).reshape(B, E, H, W)\n",
        "                return self.deproj(x)\n",
        "\n",
        "    class StyleTransformer(nn.Module):\n",
        "            def __init__(self, in_ch=3, out_ch=3, emb=96, layers=3, heads=6, ff=256, patch=16, image_size=256):\n",
        "                super().__init__()\n",
        "                self.image_size = image_size\n",
        "                self.patch = patch\n",
        "                self.h_p = image_size // patch\n",
        "                self.w_p = self.h_p\n",
        "\n",
        "                self.encoder = nn.Sequential(\n",
        "                    nn.Conv2d(in_ch, 64, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(64, emb, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "                self.patch_embed = PatchEmbed(emb, emb, patch)\n",
        "                encoder_layer = nn.TransformerEncoderLayer(d_model=emb, nhead=heads,\n",
        "                                                        dim_feedforward=ff, dropout=0.1,\n",
        "                                                        activation='gelu', batch_first=True)\n",
        "                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
        "                self.patch_unembed = PatchUnembed(emb, emb, patch)\n",
        "                self.decoder = nn.Sequential(\n",
        "                    nn.Conv2d(emb, 64, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(64, out_ch, kernel_size=3, padding=1),\n",
        "                    nn.Sigmoid()\n",
        "                )\n",
        "                N = (self.h_p * self.w_p)\n",
        "                self.pos_embed = nn.Parameter(torch.randn(1, N, emb) * 0.02)\n",
        "\n",
        "            def forward(self, x):\n",
        "                B, C, H, W = x.shape\n",
        "                assert H == self.image_size and W == self.image_size, f\"Expected {self.image_size}x{self.image_size}, got {H}x{W}\"\n",
        "                z = self.encoder(x)\n",
        "                tokens, hw = self.patch_embed(z)\n",
        "                tokens = tokens + self.pos_embed.to(tokens.device)\n",
        "                out_tokens = self.transformer(tokens)\n",
        "                z_hat = self.patch_unembed(out_tokens, hw)\n",
        "                z_hat = F.interpolate(z_hat, size=(H, W), mode='bilinear', align_corners=False)\n",
        "                out = self.decoder(z_hat)\n",
        "                return out\n",
        "\n",
        "    def load_fcc_model(weights_path=\"fcc_best_model.pt\", device=\"cuda\"):\n",
        "        model = StyleTransformer(image_size=256)\n",
        "        state_dict = torch.load(weights_path, map_location=device)\n",
        "        \n",
        "        if isinstance(state_dict, dict) and \"model_state\" in state_dict:\n",
        "            state_dict = state_dict[\"model_state\"]\n",
        "        \n",
        "        model.load_state_dict(state_dict)\n",
        "        model.to(device).eval()\n",
        "        return model\n",
        "\n",
        "    fcc_model = load_fcc_model()\n",
        "\n",
        "\n",
        "    from unsloth import FastVisionModel\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer\n",
        "    from peft import PeftModel\n",
        "\n",
        "    QWEN_model, QWEN_tokenizer = FastVisionModel.from_pretrained(\n",
        "        \"unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit\",\n",
        "        load_in_4bit=True,                   # Use 4bit quantization\n",
        "        use_gradient_checkpointing=\"unsloth\" # Enables long context with low VRAM\n",
        "    )\n",
        "\n",
        "    return QWEN_model\n",
        "    return QWEN_tokenizer\n",
        "\n",
        "\n",
        "    # 3 lora adapters, put paths in empty slots\n",
        "\n",
        "    import torch\n",
        "\n",
        "    #SAR LORA\n",
        "    sar_adapter_path = \"\"\n",
        "    lora_adapter_sar = torch.load(sar_adapter_path)\n",
        "\n",
        "    #OPTICAL LORA\n",
        "    optical_adapter_path = \"\"\n",
        "    lora_adapter_optical = torch.load(optical_adapter_path)\n",
        "\n",
        "    #YOLO LORA\n",
        "    yolo_adapter_path = \"\"\n",
        "    lora_adapter_yolo = torch.load(yolo_adapter_path)\n",
        "    \n",
        "    return lora_adapter_sar, lora_adapter_optical, lora_adapter_yolo\n",
        "\n",
        "\n",
        "    #GEOGROUND\n",
        "\n",
        "    from huggingface_hub import snapshot_download\n",
        "    import os\n",
        "\n",
        "    model_dir = \"geoground_model\"\n",
        "\n",
        "    # Remove old incomplete downloads\n",
        "    if os.path.exists(model_dir):\n",
        "        import shutil\n",
        "        shutil.rmtree(model_dir)\n",
        "\n",
        "\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Download with better error handling\n",
        "    snapshot_download(\n",
        "        repo_id=\"erenzhou/GeoGround\",\n",
        "        local_dir=model_dir,\n",
        "        local_dir_use_symlinks=False,\n",
        "        resume_download=True,\n",
        "        max_workers=4\n",
        "    )\n",
        "\n",
        "    actual_model_dir = os.path.join(model_dir, \"llava-v1.5-7b-task-lora-geoground\")\n",
        "\n",
        "    # ---------------------- LLaVA SETUP (SCRIPT SAFE) ----------------------\n",
        "    import os\n",
        "    import sys\n",
        "    import subprocess\n",
        "\n",
        "    def run_cmd(cmd):\n",
        "        \"\"\"Run a shell command cleanly.\"\"\"\n",
        "        print(f\"$ {cmd}\")\n",
        "        subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "    # 1. Clone repository if not already present\n",
        "    if not os.path.exists(\"LLaVA\"):\n",
        "        run_cmd(\"git clone https://github.com/haotian-liu/LLaVA.git\")\n",
        "\n",
        "    # 2. Add LLaVA folder to python path\n",
        "    sys.path.insert(0, os.path.abspath(\"LLaVA\"))\n",
        "\n",
        "    # 3. Install LLaVA in editable mode WITHOUT dependencies\n",
        "    old_path = os.getcwd()\n",
        "    os.chdir(\"LLaVA\")\n",
        "\n",
        "    run_cmd(\"pip install -e . --no-deps\")\n",
        "\n",
        "    os.chdir(old_path)\n",
        "\n",
        "    # ------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
