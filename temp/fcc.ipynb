{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVUbyD4g5NkP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# ==============================================================================\n",
        "#  HELPER: UNSLOTH GENERATION WRAPPER\n",
        "# ==============================================================================\n",
        "def run_unsloth_generation(model, tokenizer, image, prompt, max_tokens=512):\n",
        "    \"\"\"\n",
        "    Standardizes inference for Unsloth FastVisionModel.\n",
        "    Unsloth tokenizer handles the image processing directly.\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Prepare the messages using chat template\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"}, # Placeholder for the image\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]}\n",
        "    ]\n",
        "    \n",
        "    # 2. Format text using the tokenizer's template\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    # 3. Tokenize & Process Image (Unsloth specific syntax)\n",
        "    # Unsloth tokenizer accepts ( [image_list], text_prompt, ... )\n",
        "    inputs = tokenizer(\n",
        "        [image],\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 4. Generate\n",
        "    # We use 'fast_generate' logic implicit in the model or standard generate\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        use_cache=True,\n",
        "        temperature=0.7, # Adjust as needed\n",
        "        do_sample=False, # Deterministic for scientific accuracy\n",
        "    )\n",
        "\n",
        "    # 5. Decode response\n",
        "    # Skip the input tokens to get only the new response\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1]:], \n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#  MAIN FUNCTION: FCC PIPELINE (UNSLOTH VERSION)\n",
        "# ==============================================================================\n",
        "def run_fcc_pipeline(model, tokenizer, image_path, user_prompt, cache_dict):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model: Your loaded Unsloth model (with LoRA attached)\n",
        "        tokenizer: Your loaded Unsloth tokenizer\n",
        "        image_path: Path to the image file\n",
        "        user_prompt: The user's query\n",
        "        cache_dict: A dictionary {} passed from your main loop\n",
        "    \"\"\"\n",
        "\n",
        "    # --- PROMPTS ---\n",
        "    FCC_STAGE_1_PROMPT = \"\"\"\n",
        "    You are analyzing a False Color Composite (FCC) satellite image. FCC colors do NOT represent natural colorsâ€”ignore all color-based meaning.\n",
        "    Describe only what is visually and unambiguously present, in a short output.\n",
        "    Focus strictly on: geometric shapes, boundaries, edges, patterns, textures.\n",
        "    Do NOT name objects or land-cover types unless the shape alone makes it certain.\n",
        "    Do NOT infer, assume, or guess. Output only brief feature-level observations.\n",
        "    \"\"\"\n",
        "\n",
        "    RGB_STAGE_2_TEMPLATE = \"\"\"\n",
        "    You are analyzing an RGB image that was converted from an FCC image.\n",
        "    \n",
        "    Inputs:\n",
        "    1. The RGB image\n",
        "    2. FCC feature summary: {fcc_features}\n",
        "    \n",
        "    Task: Combine RGB geometric cues with the FCC feature summary to answer the user's request.\n",
        "    \n",
        "    User Request: {user_query}\n",
        "    \n",
        "    Rules:\n",
        "    â€¢ No hallucination, no assumptions.\n",
        "    â€¢ Use only evidence supported by geometry + FCC features.\n",
        "    â€¢ Ignore colors if they contradict the FCC summary.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"   [FCC Pipeline] Processing: {image_path}\")\n",
        "    \n",
        "    # Load Image once\n",
        "    try:\n",
        "        current_image = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return f\"Error loading image: {e}\"\n",
        "\n",
        "    # --- STAGE 1: FCC FEATURE EXTRACTION (Cached) ---\n",
        "    if image_path in cache_dict:\n",
        "        print(\"   -> âš¡ Using cached FCC features.\")\n",
        "        fcc_features = cache_dict[image_path]\n",
        "    else:\n",
        "        print(\"   -> ðŸ” Stage 1: Running FCC Feature Extraction...\")\n",
        "        fcc_features = run_unsloth_generation(\n",
        "            model, tokenizer, current_image, FCC_STAGE_1_PROMPT, max_tokens=256\n",
        "        )\n",
        "        cache_dict[image_path] = fcc_features  # Save to cache\n",
        "\n",
        "    # --- STAGE 2: RGB SYNTHESIS ---\n",
        "    # (Note: In your real logic, you might convert the image here. \n",
        "    # For now, we reuse the same image object as 'RGB' representation)\n",
        "    \n",
        "    print(\"   -> ðŸŽ¨ Stage 2: Running RGB Analysis...\")\n",
        "    \n",
        "    final_prompt = RGB_STAGE_2_TEMPLATE.format(\n",
        "        fcc_features=fcc_features, \n",
        "        user_query=user_prompt\n",
        "    )\n",
        "    \n",
        "    final_response = run_unsloth_generation(\n",
        "        model, tokenizer, current_image, final_prompt, max_tokens=512\n",
        "    )\n",
        "    \n",
        "    return final_response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
